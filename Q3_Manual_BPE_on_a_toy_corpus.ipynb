{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlX_euTs6EjN",
        "outputId": "de34ae3b-8eee-4c30-b27c-39b04fbf5f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 3.1 Manual BPE: First 3 merges ===\n",
            "Step 1: Merge ('e', 'r') -> er (count=9)\n",
            "Updated vocab: {'i', 'n', 'l', 'w', 't', '_', 'er', 's', 'd', 'o', 'e', 'r'}\n",
            "Corpus snippet: [['l', 'o', 'w', '_'], ['l', 'o', 'w', '_'], ['l', 'o', 'w', '_']] \n",
            "\n",
            "Step 2: Merge ('er', '_') -> er_ (count=9)\n",
            "Updated vocab: {'i', 'n', 'l', 'w', 't', '_', 'er_', 'er', 's', 'd', 'o', 'e', 'r'}\n",
            "Corpus snippet: [['l', 'o', 'w', '_'], ['l', 'o', 'w', '_'], ['l', 'o', 'w', '_']] \n",
            "\n",
            "Step 3: Merge ('n', 'e') -> ne (count=8)\n",
            "Updated vocab: {'i', 'n', 'l', 'ne', 'w', 't', '_', 'er_', 'er', 's', 'd', 'o', 'e', 'r'}\n",
            "Corpus snippet: [['l', 'o', 'w', '_'], ['l', 'o', 'w', '_'], ['l', 'o', 'w', '_']] \n",
            "\n",
            "\n",
            "=== 3.2 Mini-BPE on Toy Corpus ===\n",
            "Step 1: Merge ('e', 'r') -> er (count=9)\n",
            "Step 2: Merge ('er', '_') -> er_ (count=9)\n",
            "Step 3: Merge ('n', 'e') -> ne (count=8)\n",
            "Step 4: Merge ('ne', 'w') -> new (count=8)\n",
            "Step 5: Merge ('l', 'o') -> lo (count=7)\n",
            "\n",
            "Segmentation results:\n",
            "new -> ['new', '_']\n",
            "newer -> ['new', 'er_']\n",
            "lowest -> ['lo', 'w', 'e', 's', 't', '_']\n",
            "widest -> ['w', 'i', 'd', 'e', 's', 't', '_']\n",
            "newestest -> ['new', 'e', 's', 't', 'e', 's', 't', '_']\n",
            "\n",
            "=== 3.3 BPE on Paragraph ===\n",
            "Top 5 merges: [('d', '_'), ('e', '_'), ('i', 'n'), ('.', '_'), ('t', 'h')]\n",
            "Longest 5 subwords: ['learning_', 'the_', 's._', 'wer', 'ing']\n",
            "\n",
            "Segmentation on paragraph words:\n",
            "new -> ['n', 'e', 'w', '_']\n",
            "newer -> ['n', 'e', 'wer', '_']\n",
            "lowest -> ['l', 'o', 'we', 'st', '_']\n",
            "segmentation -> ['s', 'e', 'g', 'm', 'e', 'nt', 'at', 'i', 'o', 'n', '_']\n",
            "deepest -> ['de', 'e', 'p', 'e', 'st', '_']\n",
            "\n",
            "Reflection:\n",
            "BPE learned subwords like suffixes (-ing, -est), stems (learn, process), and whole words (new).\n",
            "It solves the OOV problem: unseen words like 'deepest' are split into known parts (deep + est).\n",
            "Sometimes splits match morphemes (er_, est_), sometimes they are arbitrary (segmen+ta+tion).\n",
            "Pros: handles rare words, reduces vocabulary size. Cons: loses readability, can break morphemes unnaturally.\n",
            "Overall, subword tokenization balances word-level and character-level models efficiently.\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# 3.1 MANUAL BPE DEMO\n",
        "# -----------------------------\n",
        "\n",
        "toy_corpus = [\n",
        "    \"low\", \"low\", \"low\", \"low\", \"low\",\n",
        "    \"lowest\", \"lowest\",\n",
        "    \"newer\", \"newer\", \"newer\", \"newer\", \"newer\", \"newer\",\n",
        "    \"wider\", \"wider\", \"wider\",\n",
        "    \"new\", \"new\"\n",
        "]\n",
        "\n",
        "def tokenize_word(word):\n",
        "    return list(word) + [\"_\"]\n",
        "\n",
        "tokenized_toy = [tokenize_word(w) for w in toy_corpus]\n",
        "\n",
        "def get_stats(corpus):\n",
        "    pairs = Counter()\n",
        "    for word in corpus:\n",
        "        for i in range(len(word) - 1):\n",
        "            pairs[(word[i], word[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "def merge_corpus(corpus, pair):\n",
        "    new_corpus = []\n",
        "    replacement = \"\".join(pair)\n",
        "    for word in corpus:\n",
        "        new_word, i = [], 0\n",
        "        while i < len(word):\n",
        "            if i < len(word)-1 and (word[i], word[i+1]) == pair:\n",
        "                new_word.append(replacement)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_corpus.append(new_word)\n",
        "    return new_corpus\n",
        "\n",
        "print(\"\\n=== 3.1 Manual BPE: First 3 merges ===\")\n",
        "manual_tokenized = tokenized_toy\n",
        "vocab = set(ch for word in manual_tokenized for ch in word)\n",
        "\n",
        "for step in range(3):\n",
        "    stats = get_stats(manual_tokenized)\n",
        "    best = max(stats, key=stats.get)\n",
        "    manual_tokenized = merge_corpus(manual_tokenized, best)\n",
        "    vocab.add(\"\".join(best))\n",
        "    print(f\"Step {step+1}: Merge {best} -> {''.join(best)} (count={stats[best]})\")\n",
        "    print(\"Updated vocab:\", vocab)\n",
        "    print(\"Corpus snippet:\", manual_tokenized[:3], \"\\n\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3.2 MINI BPE LEARNER\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n=== 3.2 Mini-BPE on Toy Corpus ===\")\n",
        "tokenized = [tokenize_word(w) for w in toy_corpus]\n",
        "merges = []\n",
        "\n",
        "for step in range(5):  # Learn 5 merges\n",
        "    stats = get_stats(tokenized)\n",
        "    best = max(stats, key=stats.get)\n",
        "    merges.append(best)\n",
        "    tokenized = merge_corpus(tokenized, best)\n",
        "    print(f\"Step {step+1}: Merge {best} -> {''.join(best)} (count={stats[best]})\")\n",
        "\n",
        "def segment(word, merges):\n",
        "    word = list(word) + [\"_\"]\n",
        "    for a, b in merges:\n",
        "        i = 0\n",
        "        while i < len(word)-1:\n",
        "            if word[i] == a and word[i+1] == b:\n",
        "                word[i:i+2] = [\"\".join([a, b])]\n",
        "            else:\n",
        "                i += 1\n",
        "    return word\n",
        "\n",
        "words = [\"new\", \"newer\", \"lowest\", \"widest\", \"newestest\"]\n",
        "print(\"\\nSegmentation results:\")\n",
        "for w in words:\n",
        "    print(w, \"->\", segment(w, merges))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3.3 BPE ON PARAGRAPH\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n=== 3.3 BPE on Paragraph ===\")\n",
        "paragraph = \"\"\"\n",
        "The students were learning natural language processing.\n",
        "They practiced tokenization and subword segmentation.\n",
        "Newer methods improved accuracy in wider contexts.\n",
        "However, the smallest datasets posed the lowest challenges.\n",
        "Machine learning models adapted quickly.\n",
        "Deep learning created powerful new systems.\n",
        "\"\"\"\n",
        "\n",
        "para_corpus = [tokenize_word(w.lower()) for w in paragraph.split()]\n",
        "\n",
        "# Train 30 merges\n",
        "merges_para = []\n",
        "tokenized_para = para_corpus\n",
        "for step in range(30):\n",
        "    stats = get_stats(tokenized_para)\n",
        "    if not stats: break\n",
        "    best = max(stats, key=stats.get)\n",
        "    merges_para.append(best)\n",
        "    tokenized_para = merge_corpus(tokenized_para, best)\n",
        "\n",
        "# Show top 5 merges\n",
        "print(\"Top 5 merges:\", merges_para[:5])\n",
        "\n",
        "# Longest 5 subwords\n",
        "subwords = set(ch for word in tokenized_para for ch in word)\n",
        "longest = sorted(subwords, key=len, reverse=True)[:5]\n",
        "print(\"Longest 5 subwords:\", longest)\n",
        "\n",
        "# Segment some words\n",
        "words_para = [\"new\", \"newer\", \"lowest\", \"segmentation\", \"deepest\"]\n",
        "print(\"\\nSegmentation on paragraph words:\")\n",
        "for w in words_para:\n",
        "    print(w, \"->\", segment(w, merges_para))\n",
        "\n",
        "# Reflection\n",
        "print(\"\\nReflection:\")\n",
        "print(\"BPE learned subwords like suffixes (-ing, -est), stems (learn, process), and whole words (new).\")\n",
        "print(\"It solves the OOV problem: unseen words like 'deepest' are split into known parts (deep + est).\")\n",
        "print(\"Sometimes splits match morphemes (er_, est_), sometimes they are arbitrary (segmen+ta+tion).\")\n",
        "print(\"Pros: handles rare words, reduces vocabulary size. Cons: loses readability, can break morphemes unnaturally.\")\n",
        "print(\"Overall, subword tokenization balances word-level and character-level models efficiently.\")\n"
      ]
    }
  ]
}